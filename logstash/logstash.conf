input {
  kafka {
    bootstrap_servers => "kafka:9092"  # Kafka 브로커 주소
    topics => ["multi-test-topic"]      # Logstash가 읽을 Kafka 토픽
    group_id => "logstash-consumer"     # Logstash 전용 Consumer 그룹
    auto_offset_reset => "earliest"     # 가장 처음부터 데이터 읽기
    enable_auto_commit => false
  }
}

# 필터 부분에서는 데이터 변환 규칙을 추가할 수 있습니다.
# 예를 들어, 특정 필드를 추가하거나 변환할 수 있습니다.
filter {
  mutate {
    add_field => { "processed_by" => "logstash" }
  }
}

output {
  # 데이터 출력 1: 콘솔에 출력
  stdout { codec => json }

  # 데이터 출력 2: 파일에 저장
  file {
    path => "/usr/share/logstash/data/kafka_output.log"
    codec => json
  }
  
  # 데이터 출력 3: Elasticsearch로 전송 (원할 경우 활성화)
  # elasticsearch {
  #   hosts => ["http://elasticsearch:9200"]
  #   index => "kafka-data"
  # }
}
